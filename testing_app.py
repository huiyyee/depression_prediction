# -*- coding: utf-8 -*-
"""testing_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rk665VsnkGfnWcHzzFBypYSW0fgei4Id

import pickle
import os

Specify the path to save the files
save_path = 'D:/Assignments/FYP/Coding/FLAVORVERSE/backend'

Ensure the path exists
os.makedirs(save_path, exist_ok=True)

Save the sampled dataframe
with open(os.path.join(save_path, 'sampled_df.pkl'), 'wb') as f:
    pickle.dump(sampled_df, f)

Save the vectorizer and TF-IDF matrix for content-based filtering
with open(os.path.join(save_path, 'vectorizer.pkl'), 'wb') as f:
    pickle.dump(vectorizer, f)

with open(os.path.join(save_path, 'tfidf_matrix.pkl'), 'wb') as f:
    pickle.dump(tfidf_matrix, f)

Save the best collaborative filtering model
with open(os.path.join(save_path, 'best_algo.pkl'), 'wb') as f:
    pickle.dump(best_algo, f)
"""

import streamlit as st
import pickle
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from textblob import TextBlob, Translator
from textblob import TextBlob
from Bio.SeqUtils import sequences

# Load the pickled files
save_path = '/Users/huiyee/Downloads/Study/Year3Sem2/FYP Project/backend'

with open(os.path.join(save_path, 'new_df.pkl'), 'rb') as f:
    new_df = pickle.load(f)

with open(os.path.join(save_path, 'tfidf_matrix.pkl'), 'rb') as f:
    vectorizer = pickle.load(f)

with open(os.path.join(save_path, 'best_dt_classifier.pkl'), 'rb') as f:
    best_algo = pickle.load(f)

# Define preprocessing functions
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = expand_contractions(text)  # Expand contractions
    text = transform_chat_words(text)  # Transform chat words
    text = translate_to_english(text)  # Translate to English
    text = remove_urls(text)  # Remove URLs
    text = remove_html_tags(text)  # Remove HTML tags
    text = remove_punctuation(text)  # Remove punctuation
    text = remove_digits(text)  # Remove digits
    text = remove_stopwords(text)  # Remove stopwords
    tokens = word_tokenize(text)  # Tokenize
    tokens = remove_special_symbols(tokens)  # Remove special characters
    tokens = remove_empty_tokens(tokens)  # Remove empty tokens
    tokens = lemmatize_text(tokens)  # Lemmatize tokens
    text = ' '.join(tokens)  # Join tokens back into text
    return text

# Function to expand contractions
def expand_contractions(text):
    return contractions.fix(text)

# Chat words mapping dictionary
chat_word_mapping = {
    # Your chat word mappings here
       "afaik": "as far as i know",
    "afk": "away from keyboard",
    "asap": "as soon as possible",
    "atk": "at the keyboard",
    "atm": "at the moment",
    "a3": "anytime, anywhere, anyplace",
    "bak": "back at keyboard",
    "bbl": "be back later",
    "bbs": "be back soon",
    "bfn": "bye for now",
    "b4n": "bye for now",
    "brb": "be right back",
    "brt": "be right there",
    "btw": "by the way",
    "b4": "before",
    "b4n": "bye for now",
    "cu": "see you",
    "cul8r": "see you later",
    "cya": "see you",
    "faq": "frequently asked questions",
    "fc": "fingers crossed",
    "fwiw": "for what it's worth",
    "fyi": "for your information",
    "gal": "get a life",
    "gg": "good game",
    "gn": "good night",
    "gmta": "great minds think alike",
    "gr8": "great!",
    "g9": "genius",
    "ic": "i see",
    "icq": "i seek you (also a chat program)",
    "ilu": "ilu: i love you",
    "imho": "in my honest/humble opinion",
    "imo": "in my opinion",
    "iow": "in other words",
    "irl": "in real life",
    "kiss": "keep it simple, stupid",
    "ldr": "long distance relationship",
    "lmao": "laugh my ass off",
    "lol": "laughing out loud",
    "ltns": "long time no see",
    "l8r": "later",
    "mte": "my thoughts exactly",
    "m8": "mate",
    "nrn": "no reply necessary",
    "oic": "oh i see",
    "pita": "pain in the ass",
    "prt": "party",
    "prw": "parents are watching",
    "rofl": "rolling on the floor laughing",
    "roflol": "rolling on the floor laughing out loud",
    "rotflmao": "rolling on the floor laughing my ass off",
    "sk8": "skate",
    "stats": "your sex and age",
    "asl": "age, sex, location",
    "thx": "thank you",
    "ttfn": "ta-ta for now!",
    "ttyl": "talk to you later",
    "u2": "you too",
    "u4e": "yours for ever",
    "wb": "welcome back",
    "wtf": "what the fuck",
    "wtg": "way to go!",
    "wuf": "where are you from?",
    "w8": "wait...",
    "7k": "sick:-d laughter",
}

# Function to transform chat words
def transform_chat_words(text):
    for chat_word, expanded_form in chat_word_mapping.items():
        text = text.replace(chat_word, expanded_form)
    return text

# Function to translate text to English
def translate_to_english(text):
    try:
        if detect_langs(text) != 'en':
            translator = Translator(to_lang="en")
            translation = translator.translate(text)
        else:
            translation = text
        return translation
    except Exception as e:
        print(f"Translation error: {e}")
        return None

# Function to remove URLs
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub('', text)

# Function to remove HTML tags
def remove_html_tags(text):
    clean_text = re.sub(r'<[^>]*>', '', text)
    return clean_text

# Function to remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove stopwords
stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

# Function to remove special symbols
def remove_special_symbols(tokens):
    cleaned_tokens = []
    for token in tokens:
        pattern = r'[^a-zA-Z0-9\s]'
        cleaned_token = re.sub(pattern, '', token)
        cleaned_tokens.append(cleaned_token)
    return cleaned_tokens

# Function to remove empty tokens
def remove_empty_tokens(tokens):
    return [token for token in tokens if token.strip()]

# Function to lemmatize text using spaCy
nlp = spacy.load('en_core_web_sm')
def lemmatize_text(tokens):
    doc = nlp(' '.join(tokens))
    lemmatized_tokens = [token.lemma_ for token in doc]
    return lemmatized_tokens

# Title and description
st.title("Depression Prediction App")
st.write("""
This app uses machine learning to predict the likelihood of depression based on text input.
Enter some text related to your thoughts and feelings to get a prediction.
""")

# Input text area
user_input = st.text_area("Enter your text here:")

# Predict button
if st.button("Predict"):
    if user_input:
        # Vectorize the user input
        user_input_tfidf = vectorizer.transform([user_input])

        # Predict using the loaded model
        prediction = best_algo.predict(user_input_tfidf)

        # Display the result
        if prediction[0] == 1:
            st.write("The model predicts that the text indicates signs of depression.")
        else:
            st.write("The model predicts that the text does not indicate signs of depression.")
    else:
        st.write("Please enter some text to get a prediction.")